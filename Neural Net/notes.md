

---

### Insight on Backpropagation and Forward Propagation (BP & FP)

Backpropagation (BP) and forward propagation (FP) are not separate algorithms but are closely connected. Understanding their relationship is key to creating efficient training algorithms.

**Forward Propagation (FP):**  
FP is conceptually straightforwardâ€”it processes an input through sequential layers (modules) to produce an output.

**Backpropagation (BP):**  
BP is more complex, involving multivariable calculus, Jacobians, and the chain rule to calculate how to adjust weights during training.

The mathematical formulation of BP is as follows:

1. Assume a network with an input layer, \( n \) hidden layers, and an output layer (logits).
2. Compute the gradient of the loss with respect to the output layer.
3. Compute the gradient of the loss with respect to the \( n \)-th hidden layer.
4. For \( k = n, n-1, \dots, 2 \):
   - Compute the gradient between layer \( k \) and layer \( k-1 \).
   - Compute the gradient of the loss with respect to layer \( k-1 \) using the chain rule and previously calculated gradients.
5. For \( k = n+1, \dots, 1 \):
   - Compute the gradient of the loss with respect to the weights in layer \( k \).

**Intuitive Explanation:**  
Backpropagation sends a signal backward through the network, related to the loss, which adjusts the parameters (weights and biases) of each module. As this signal passes through each module, it is modified by the module's parameters and continues flowing backward until all parameters are appropriately updated.

**Mathematical Formulation in Context:**

Consider two types of modules:
- **Linear Modules (weights and biases)**
- **Functional Modules (e.g., ReLU)**

When the loss signal enters a module, we refer to it as the `grad_input` signal. When it exits the module after modification, it's called the `grad_output` signal.

Only the linear modules are updated during backpropagation. When `grad_input` enters a linear module, it interacts with the FP input to determine how the module's parameters should change. The `grad_input` is then modified by the module's parameters (before change) to produce a `grad_output` for the next module.

For functional modules, although no parameters are updated, a `grad_output` is still generated by the interaction between `grad_input` and the FP input.

### Understanding loss

The likelihood function and binary cross-entropy are closely related concepts in statistics and machine learning, especially in the context of binary classification problems.

### Likelihood Function

**Likelihood Function**:
- In statistics, the likelihood function represents the probability of observing the given data under different parameter values of a statistical model.
- For a given statistical model with parameters \(\theta\), and observed data \(D\), the likelihood function \(L(\theta | D)\) is given by:
  \[
  L(\theta | D) = P(D | \theta)
  \]
  This function evaluates how probable the observed data is given different values of the parameters.

### Binary Cross-Entropy Loss

**Binary Cross-Entropy Loss**:
- In machine learning, especially in binary classification, binary cross-entropy (also known as log loss) is a commonly used loss function.
- It measures the difference between the true labels and the predicted probabilities produced by the model.
- For binary classification, the binary cross-entropy loss function is given by:
  \[
  \text{Loss}(y, \hat{y}) = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
  \]
  where \(y\) is the true label (0 or 1), and \(\hat{y}\) is the predicted probability of the positive class.

### Relationship Between Likelihood Function and Binary Cross-Entropy

The connection between the likelihood function and binary cross-entropy comes from the fact that minimizing binary cross-entropy is equivalent to maximizing the likelihood function in the context of binary classification.

**Detailed Relationship**:

1. **Likelihood Function for Binary Classification**:
   - Consider a binary classification model where the probability of the positive class is \(\hat{y}\), and the probability of the negative class is \(1 - \hat{y}\).
   - For a single data point with true label \(y\), the likelihood of the observed label given the predicted probability \(\hat{y}\) is:
     \[
     P(y | \hat{y}) = \hat{y}^y \cdot (1 - \hat{y})^{(1 - y)}
     \]

2. **Log-Likelihood Function**:
   - To simplify calculations and because it's often more convenient, we work with the log-likelihood function. The log-likelihood for a single data point is:
     \[
     \log P(y | \hat{y}) = y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})
     \]

3. **Binary Cross-Entropy Loss**:
   - The binary cross-entropy loss function is the negative log-likelihood. Therefore:
     \[
     \text{Loss}(y, \hat{y}) = - \log P(y | \hat{y}) = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
     \]
   - By minimizing the binary cross-entropy loss, you are effectively maximizing the likelihood of the observed data.

### Summary

- **Likelihood Function**: Represents the probability of the observed data given the parameters of the model.
- **Binary Cross-Entropy**: Measures how well the predicted probabilities match the true binary labels. It is derived from the log-likelihood of the binary classification model.
- **Connection**: Minimizing the binary cross-entropy loss function is equivalent to maximizing the likelihood function in binary classification scenarios.

### 1. **Entropy**

**Entropy** is a measure from information theory that quantifies the uncertainty or randomness in a probability distribution. It provides a way to understand how unpredictable or chaotic a distribution is.

- **Mathematical Definition**:
  For a discrete random variable \(X\) with probability mass function \(P(X)\), the entropy \(H(X)\) is defined as:
  \[
  H(X) = - \sum_{i} P(x_i) \log P(x_i)
  \]
  where \(x_i\) are the possible values of \(X\) and \(P(x_i)\) is the probability of \(x_i\).

- **Interpretation**:
  - Higher entropy means higher uncertainty or randomness.
  - Lower entropy means the distribution is more predictable or deterministic.

### 2. **KL-Divergence (Kullback-Leibler Divergence)**

**KL-Divergence** measures how one probability distribution diverges from a second, reference probability distribution. It quantifies the difference between two distributions.

- **Mathematical Definition**:
  Given two probability distributions \(P\) (the true distribution) and \(Q\) (the approximating distribution), the KL-divergence \(D_{KL}(P \parallel Q)\) is defined as:
  \[
  D_{KL}(P \parallel Q) = \sum_{i} P(x_i) \log \frac{P(x_i)}{Q(x_i)}
  \]
  for discrete distributions. For continuous distributions, the sum is replaced by an integral.

- **Interpretation**:
  - It is a non-negative measure. \(D_{KL}(P \parallel Q) = 0\) if and only if \(P\) and \(Q\) are identical.
  - It is not symmetric, meaning \(D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)\).

### 3. **Sigmoid Function**

The **sigmoid function** is an activation function commonly used in binary classification problems. It maps any real-valued number into the (0, 1) interval.

- **Mathematical Definition**:
  The sigmoid function \( \sigma(z) \) is defined as:
  \[
  \sigma(z) = \frac{1}{1 + e^{-z}}
  \]
  where \(z\) is the input to the function.

- **Properties**:
  - **Range**: \( (0, 1) \)
  - **Shape**: S-shaped curve.
  - **Derivative**: The derivative of the sigmoid function is:
    \[
    \sigma'(z) = \sigma(z) (1 - \sigma(z))
    \]

### 4. **Softmax Function**

The **softmax function** generalizes the sigmoid function to multi-class classification problems. It converts logits (raw prediction scores) into probabilities by emphasizing the largest logits.

- **Mathematical Definition**:
  For a vector of raw scores \( \mathbf{z} = [z_1, z_2, \ldots, z_K] \), the softmax function for the \(i\)-th class is:
  \[
  \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
  \]
  where \(K\) is the number of classes.

- **Properties**:
  - **Range**: \( (0, 1) \)
  - **Sum**: The outputs sum up to 1, making them valid probabilities.

### 5. **Loss Functions**

**Loss functions** quantify the difference between the predicted outputs of a model and the true labels. They are used to train models by minimizing this difference.

#### **Cross-Entropy Loss**:

- **Binary Cross-Entropy**:
  For binary classification, the binary cross-entropy loss function is:
  \[
  \text{Loss}_{\text{binary}}(y, \hat{y}) = - [y \log(\hat{y}) + (1 - y) \log(1 - \hat{y})]
  \]
  where \( y \) is the true label and \( \hat{y} \) is the predicted probability.

- **Categorical Cross-Entropy**:
  For multi-class classification, categorical cross-entropy loss is:
  \[
  \text{Loss}_{\text{categorical}}(y, \hat{y}) = - \sum_{i} y_i \log(\hat{y}_i)
  \]
  where \( y_i \) is 1 if \( i \) is the true class and 0 otherwise, and \( \hat{y}_i \) is the predicted probability for class \( i \).

### **Relationship Between Concepts**

- **Entropy and Loss Functions**:
  - **Entropy** is used to quantify the amount of uncertainty or information in a probability distribution.
  - **Cross-Entropy Loss** measures how well the predicted probability distribution matches the true distribution. Minimizing cross-entropy loss during training effectively minimizes the entropy between the predicted and true distributions.

- **KL-Divergence and Cross-Entropy Loss**:
  - KL-divergence between the true label distribution \(P\) and predicted distribution \(Q\) can be interpreted in terms of cross-entropy:
    \[
    D_{KL}(P \parallel Q) = H(P, Q) - H(P)
    \]
    where \(H(P, Q)\) is the cross-entropy between \(P\) and \(Q\), and \(H(P)\) is the entropy of the true distribution \(P\). Minimizing cross-entropy is equivalent to minimizing KL-divergence if the true distribution is fixed.

- **Sigmoid and Cross-Entropy**:
  - In binary classification, the sigmoid function is used to predict probabilities, and the binary cross-entropy loss measures the discrepancy between the predicted probabilities and true labels.

- **Softmax and Cross-Entropy**:
  - In multi-class classification, the softmax function converts logits into probabilities, and the categorical cross-entropy loss measures the discrepancy between these predicted probabilities and true labels.

### Summary

- **Entropy** quantifies uncertainty.
- **KL-Divergence** measures the difference between two distributions.
- **Sigmoid** is used for binary classification, mapping logits to probabilities.
- **Softmax** generalizes sigmoid to multi-class problems.
- **Cross-Entropy Loss** measures the performance of classification models by comparing predicted probabilities to true labels and is derived from entropy and KL-divergence concepts.